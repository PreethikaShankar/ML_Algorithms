{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "K-Means_-Women_E-Commerce_Clothing_Reviews.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPYnf2pzqhqDsnuJuDha4i8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PreethikaShankar/ML_Algorithms/blob/main/K_Means__Women_E_Commerce_Clothing_Reviews.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmdupaLx6YmA"
      },
      "source": [
        "#__K-means Clustering__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-sqBR846hWj"
      },
      "source": [
        "## Table of Content\n",
        "1. [Introduction to Clustering](#section1)<br>\n",
        "    - 1.1 [Overview: What is Clustering?](#section101)<br>\n",
        "    - 1.2 [Types of Clustering](#section103)<br>\n",
        "    - 1.3 [Types of Clustering Algorithms](#section103)<br>\n",
        "    - 1.4 [Introduction to K-means Algorithm](#section104)<br>\n",
        "    - 1.5 [Business Cases](#section105)<br>\n",
        "    - 1.6 [Algorithm](#section106)<br>\n",
        "    - 1.7 [Choosing K](#section107)<br>\n",
        "    - 1.8 [How good is K-Means](#section108)<br>\n",
        "2. [Use Case - Women E-commerce Clothing Reviews](#section2)<br>\n",
        "3. [Data loading and description](#section3)<br>\n",
        "4. [Data Interpretation and Visualization](#section4)<br>\n",
        "5. [Train test split](#section5)<br>\n",
        "6. [Conclusions](#section6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKA6qQBT6q65"
      },
      "source": [
        "### Overview : What is Clustering?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODu8Feqy62e2"
      },
      "source": [
        "Clustering is the task of __dividing the population__ or __data points__ into _a number of groups_ such that _data points in the same groups are more similar to other data points in the same group_ than those in other groups. \n",
        "> __In simple words, the aim is to segregate groups with similar traits and assign them into clusters__. \n",
        "\n",
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeans2.png'>\n",
        "\n",
        "Let’s understand this with an example.<br>\n",
        "1. Suppose, you are the __head of a rental store__ and wish to __understand preferences__ of your costumers to __scale up your business__. \n",
        "2. Is it possible for you to look at _details of each costumer and devise a unique business strategy_ for each one of them? __Definitely not__. \n",
        "3. But, what you can do is to __cluster__ all of your _costumers into say 10 groups based on their purchasing habits_ and use a __separate strategy__ for _costumers in each of these 10 groups_. \n",
        "4. This is what we call __clustering__."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x9iyVpF69AI"
      },
      "source": [
        "### Types of Clustering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Etrx_itS6_7m"
      },
      "source": [
        "Clustering can be divided into two subgroups :\n",
        "\n",
        "1. __Hard Clustering__: In hard clustering, each data point __either belongs to a cluster completely or not__.<br> _For example_, in the above example each customer is put into one group out of the 10 groups.<br><br>\n",
        "\n",
        "2. __Soft Clustering__: In soft clustering, instead of putting each data point into a separate cluster, a __probability or likelihood__ of that data point to be in those clusters is assigned.<br>_For example_, from the above scenario each costumer is assigned a probability to be in either of 10 clusters of the retail store.\n",
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeans3.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oMwxWNKM7BcH"
      },
      "source": [
        "### Types of Clustering Algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWSmoby_7FvR"
      },
      "source": [
        "Since the task of clustering is subjective, this means that can be used for achieving this goal are plenty. Every methodology follows a different set of rules for defining the ‘similarity’ among data points. In fact, there are more than 100 clustering algorithms known. But few of the algorithms are used popularly, let’s look at them in detail:\n",
        "- __Connectivity models__: These models are based on the notion that the data points _closer in data space __exhibit more similarity__ to each other __than__ the data points __lying farther__ away_. These models can follow two approaches.<br> Examples of these models are __hierarchical clustering algorithm__ and its variants.<br><br>\n",
        "- __Centroid models__: These are __iterative clustering algorithms__ in which the notion of similarity is derived by the __closeness of a data point to the centroid of the clusters__. K-Means clustering algorithm is a popular algorithm that falls into this category. These models run iteratively to find the __local optima__.<br><br>\n",
        "- __Distribution models__: These clustering models are based on the notion of __how probable is it that all data points in the cluster belong to the same distribution__ (For example: Normal, Gaussian). These models often suffer from __overfitting__. A popular example of these models is __Expectation-maximization algorithm__ which uses multivariate normal distributions.<br><br>\n",
        "- __Density Models__: These models search the data space for __areas of varied density of data points__ in the data space. It isolates various different density regions and assign the data points within these regions in the same cluster. Popular examples of density models are __DBSCAN and OPTICS__.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6yfEZC87H3r"
      },
      "source": [
        "### Introduction to K-means Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE2oJtJA7NBa"
      },
      "source": [
        "K-means clustering is a type of __unsupervised learning__, which is used when you have __unlabeled data (i.e., data without defined categories or groups)__. \n",
        "> __The goal of this algorithm is to find groups in the data, with the number of groups represented by the variable K__. <br>\n",
        "\n",
        "- The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. <br>\n",
        "- Data points are clustered based on feature similarity.<br>\n",
        "\n",
        "The results of the __K-means clustering algorithm__ are:\n",
        "1. The centroids of the K clusters, which can be used to label new data\n",
        "2. Labels for the training data (each data point is assigned to a single cluster)![image.png](https://cdn-images-1.medium.com/max/716/1*WkU1q0Cuha2QKU5JnkcZBw.gif)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua1kqBTB7PZp"
      },
      "source": [
        "### Business Cases\n",
        "\n",
        "The K-means clustering algorithm is used to __find groups__ which have not been __explicitly labeled__ in the data. This can be used to confirm business assumptions about what types of groups exist or to identify unknown groups in complex data sets.<br>\n",
        "This is a versatile algorithm that can be used for any type of grouping. Some examples of __use cases__ are:\n",
        "- Behavioral Segmentation\n",
        "- Inventory Categorization\n",
        "- Sorting Sensor measurements\n",
        "- Detecting bots and anomalies\n",
        "- Computer Vision\n",
        "- Astronomy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cUObXyP7WoP"
      },
      "source": [
        "### Algorithm\n",
        "\n",
        "To start with k-means algorithm, you first have to randomly initialize points called the cluster centroids (K).<br>\n",
        "There are different methods to initialize the k value in the k-means algorithm:\n",
        "- __Forgy__ - Randomly assigning K centroids in our data set.\n",
        "- __Random Partition__: Assigning each data point to a cluster randomly, and then proceeding to evaluation of centroid positions of each cluster.\n",
        "- __KMeans++__: Used for samlla data sets.\n",
        "- __Canopy Clustering__: Unsupervised pre-clustering algorithm used as preprocessing step for K-means or any Heirarchichal Clustering. It helps in speeding up clustering operations on large data sets.\n",
        "K-means is an __iterative algorithm__ and it does __two__ steps:<br>\n",
        "### __1. Cluster Assignment__\n",
        "The algorithm goes through each of the data points and depending on which cluster is closer, It assigns the data points to one of the three cluster centroids.\n",
        "\n",
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeans4.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV-ozoEC7kQZ"
      },
      "source": [
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeans5.gif'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO74mOrMEvM_"
      },
      "source": [
        "### 2. Move Centroid\n",
        "Here, K-means moves the centroids to the average of the points in a cluster. In other words, the algorithm calculates the average of all the points in a cluster and moves the centroid to that average location.\n",
        "\n",
        "This process is repeated until there is no change in the clusters (or possibly until some other stopping condition is met). K is chosen randomly or by giving specific initial starting points by the user.<br>\n",
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeans6.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg2wMZYhEzef"
      },
      "source": [
        "#### Let's see how actually the k is taken into consideration.\n",
        "\n",
        "As a starting point, you tell your model how many clusters it should make. First the model picks up K, (let K = 3) datapoints from the dataset. These datapoints are called cluster centroids.\n",
        "\n",
        "Now there are different ways you to initialize the centroids, you can either choose them at random — or sort the dataset, split it into K portions and pick one datapoint from each portion as a centriod.\n",
        "\n",
        "#### Assigning clusters to datapoints\n",
        "From here on wards, the model performs calculations on it’s own and assigns a cluster to each datapoint. Your model would calculate the distance between the datapoint & all the centroids, and will be assigned to the cluster with the nearest centroid. Again, there are different ways you can calculate this distance; all having their pros and cons. Usually we use the L2 distance.\n",
        "\n",
        "The picture below shows how to calculate the L2 distance between the centeroid and a datapoint. Every time a datapoint is assigned to a cluster the following steps are followed.\n",
        "\n",
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeans7.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z_q2ithE3SG"
      },
      "source": [
        "#### Updating centroids\n",
        "Because the initial centroids were chosen arbitrarily, your model the updates them with new cluster values. The new value might or might not occur in the dataset, in fact, it would be a coincidence if it does. This is because the updated cluster centorid is the average or the mean value of all the datapoints within that cluster.\n",
        "\n",
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeasn8.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giia3AGcE6c4"
      },
      "source": [
        "## Choosing K\n",
        "\n",
        "One of the metrics that is commonly used to compare results across different values of 'K' is the __mean distance between data points and their cluster centroid__. \n",
        "- Since _increasing the number of clusters will always reduce the distance to data points, increasing K will always decrease this metric, to the extreme of reaching zero when K is the same as the number of data points_. \n",
        "Thus, this metric cannot be used as the sole target. \n",
        "Instead, mean distance to the centroid as a function of K is plotted and the __\"elbow point,\"__ where the __rate of decrease sharply shifts__, can be used to roughly __determine K__.\n",
        "\n",
        "A number of other techniques exist for validating K, including __cross-validation__, __information criteria__, the __information theoretic jump method__, the __silhouette method__, and the __G-means algorithm__. In addition, monitoring the distribution of data points across groups provides __insight__ into how the _algorithm is splitting the data for each K_.\n",
        "\n",
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeans9.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcJT3f4BE_7p"
      },
      "source": [
        "## __How good is K-means?__\n",
        "\n",
        "#### __Pros__\n",
        "- It’s a simple technique to understand. Even with the math behind it!\n",
        "- Quite fast for small data-sets.\n",
        "\n",
        "#### __Cons__\n",
        "- For large data-sets and large number of features, it gets computationally expensive.\n",
        "- If the data-set is sparse, then we may not get a good-quality clustering.\n",
        "- At times, it’s difficult to determine the number of clusters for K-Means.\n",
        "- It’s sensitive to outliers, so you should consider scaling your features before implementing K-Means.\n",
        "- Since the centroids get randomly initialised, the final centroids for the clusters may vary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Z7r7K93FQXd"
      },
      "source": [
        "### Introduction to Heirarchial Clustering\n",
        "\n",
        "It is a type of __connectivity model__ clustering which is based on the fact that data points that are __closer to each other__ are __more similar__ than the data points lying far away in a data space.\n",
        "\n",
        "As the name speaks for itself, the hierarchical clustering forms the __hierarchy of the clusters__ that can be studied by visualising __dendogram__.\n",
        "\n",
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeans10.png'>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRJS05b4FYO_"
      },
      "source": [
        "### How to measure closeness of points?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCS0DM6NFZDG"
      },
      "source": [
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeans11.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kgOeIa-HFb1o"
      },
      "source": [
        "### How to calculate distance between two clusters?\n",
        "\n",
        "1. __Centroid Distance__: Euclidean distance between mean of data points in the two clusters\n",
        "2. __Minimum Distance__: Euclidean distance between two data points in the two clusters that are closest to each other\n",
        "3. __Maximum Distance__: Euclidean distance between two data points in the two clusters that are farthest to each other<br>\n",
        "\n",
        "\n",
        "- __Focus on Centroid Distance right now!__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbGETzJZFjQZ"
      },
      "source": [
        "### Algorithm Explained\n",
        "\n",
        "1. Let there be __N__ data points. Firstly, these N data points are assigned to N different clusters with one data point in each cluster.\n",
        "2. Then, two data points with __minimum euclidean distance__ between them are merged into a single cluster.\n",
        "3. Then, two clusters with __minimum centroid distance__ between them are merged into a single cluster.\n",
        "4. This process is repeated until we are left with a single cluster, hence forming hierarchy of clusters.\n",
        "\n",
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeans12.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cxb7TEVgFHMN"
      },
      "source": [
        "### How many clusters to form?\n",
        "\n",
        "1. __Visualising dendogram__: Best choice of no. of clusters is no. of vertical lines that can be cut by a horizontal line, that can transverse maximum distance vertically without intersecting other cluster. \n",
        "    For eg., in the below case, best choice for no. of clusters will be __4__.\n",
        "2. __Intuition__ and prior knowledge of the data set.\n",
        "\n",
        "<img src = 'https://raw.githubusercontent.com/insaid2018/Term-3/master/Images/k-means/kmeans13.png'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wb99ObbF1Ej"
      },
      "source": [
        "### __Good Cluster Analysis__\n",
        "\n",
        "- __Data-points within same cluster share similar profile__: Statistically, check the standard deviation for each input variable in each cluster. A perfect separation in case of cluster analysis is rarely achieved. Hence, even __one standard deviation__ distance between two cluster means is considered to be a good separation.\n",
        "- __Well spread proportion of data-points among clusters__: There are no standards for this requirement. But a minimum of 5% and maximum of 35% of the total population can be assumed as a safe range for each cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGEE54dGF8BL"
      },
      "source": [
        "## Use Case : Women E-Commerce Clothing Reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-rpPdIAF_ZG"
      },
      "source": [
        "# Consumer Segmentation\n",
        "Consumer segmentation is the practice of dividing a customer base into groups. In each of these groups the individuals are similar to each other in some ways. This practice allows a business to create a group specific campaign or digital advertisement. This not only increases the effectiveness of the business's marketing efforts, it helps in reducing the marketing costs by reducing spends on certain audience groups that might react differently to a campaign. Segmentation also enables businesses to focus marketing activities more on certain platforms based on the characteristics of the segments.\n",
        "Though segmentation is used to create homogenous groups of individuals, we will use it to understand consumer behaviour in this notebook.\n",
        "\n",
        "In this script, we look at one of the most popular  segmentation algorithms; the k-means clustering algorithm.  k-means clustering is an *unsupervised learning* algorithm that groups data into $K$ number of sets by using an iterative process. For each cluster, the centroid is chosen in such a way that the distance between the centroid and the data points in the cluster is minimized. Before we carry on, we drop variables that are nominal in nature. We also try to convert the review text to sentiment scores as K Means only works with numerical data.\n",
        "\n",
        "# Library Load\n",
        "\n",
        "Here we will be using the pandas library to do data processing. The sklearn modules also help with necessary scaling of variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pzo_Xs_uGmR-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.mixture import GaussianMixture #For GMM clustering\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj7K7TwGGrgo"
      },
      "source": [
        "review_data = pd.read_csv(\"https://raw.githubusercontent.com/insaid2018/Term-3/master/Data/CaseStudy/Womens%20Clothing%20E-Commerce%20Reviews.csv\")\n",
        "review_data.dropna(inplace=True)\n",
        "#review_data=review_data.drop(['Unnamed: 0', 'Clothing ID'],axis=1)\n",
        "print(review_data.dtypes)\n",
        "review_data.head(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WZGJiWsG8bA"
      },
      "source": [
        "__In the next portion of the notebook we remove all the categorical variables.__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvOmioUbHDzN"
      },
      "source": [
        "# remove all the columns that are categorical variables\n",
        "review_data_k_means=review_data.drop(['Unnamed: 0', 'Clothing ID','Class Name','Department Name','Title','Division Name','Recommended IND'],axis=1)\n",
        "review_data_k_means"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7B2_j2vH6yR"
      },
      "source": [
        "Each product has a text review associated with it. Text can be converted to a numerical value using sentiment scores. One way to do this is to use predefined sentiment lexicons and match them accordingly. For this example, we will use the AFINN lexicon.\n",
        "\n",
        "As the Afinn library is not available on Kaggle, I have tried to post this portion of the code in raw format. A negative value denotes negative sentiment and a positive score indicates positive sentiments. The scores that are returned are then stored in a `.txt` format and called using the `pickle.load` function as seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4kGtyHcH7mk"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/abromberg/sentiment_analysis/master/AFINN/AFINN-111.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-OVQypuO-3l"
      },
      "source": [
        "# converting review text to sentiment score\n",
        "from afinn import Afinn\n",
        "afinn= Afinn()\n",
        "review_data_k_means['Review Text'] = review_data_k_means['Review Text'].str.lower()\n",
        "review_data_k_means['sent_score'] = review_data_k_means.apply(lambda row: afinn.score(row['Review Text']), axis=1)\n",
        "\n",
        "sent_score = review_data_k_means['sent_score'].values\n",
        "with open(\"sent.txt\", \"wb\") as fp:   #Pickling\n",
        "    pickle.dump(sent_score, fp)\n",
        "\n",
        "with open(\"sent.txt\", \"rb\") as fp:   # Unpickling\n",
        "    b = pickle.load(fp)\n",
        "\n",
        "#print(b)\n",
        "\n",
        "\n",
        "adding the minimum value of sentiment score so as to remove negative sentiment scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaUmOOm_PGFc"
      },
      "source": [
        "Each product has a text review associated with it. Text can be converted to a numerical value using sentiment scores. One way to do this is to use predefined sentiment lexicons and match them accordingly. For this example, we will use the AFINN lexicon.\n",
        "\n",
        "As the Afinn library is not available on Kaggle, I have tried to post this portion of the code in raw format. A negative value denotes negative sentiment and a positive score indicates positive sentiments. The scores that are returned are then stored in a `.txt` format and called using the `pickle.load` function as seen below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0kU4ihoO5aN"
      },
      "source": [
        "with open(\"AFINN-111.txt\",'rb') as fp:\n",
        " data = fp.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvV3CKS6POoz"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu9MmT2jPSHa"
      },
      "source": [
        "str(data).split(\"\\\\n\")[100].split(\"\\\\t\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oy10vCzaPV_m"
      },
      "source": [
        "# str(data).split(\"\\\\n\")[10].split(\"\\\\t\")\n",
        "#!wget https://raw.githubusercontent.com/abromberg/sentiment_analysis/master/AFINN/AFINN-111.txt\n",
        "d= {}\n",
        "\n",
        "for each_element in str(data).split(\"\\\\n\"):\n",
        "  d[each_element.split(\"\\\\t\")[0]] = each_element.split(\"\\\\t\")[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGtCIPZLPZt9"
      },
      "source": [
        "d['abducted']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktFJ2mf7QDOx"
      },
      "source": [
        "int(d['accusing'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rluL7bM-QIvt"
      },
      "source": [
        "import numpy as np\n",
        "def getSentiment(eachRow):\n",
        "  TotalSum = []\n",
        "  for eachWord in eachRow.split():\n",
        "    if eachWord.lower() in d:   # if it is found in the dictionary \n",
        "      TotalSum.append(int(d[eachWord.lower()]))\n",
        "    else:\n",
        "      TotalSum.append(0)\n",
        "  return np.sum(TotalSum)\n",
        "  # return TotalSum\n",
        "\n",
        "\n",
        "review_data_k_means['Sentiment'] = review_data_k_means['Review Text'].apply(getSentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wiy4X4mQdPW"
      },
      "source": [
        "review_data_k_means"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}